# 第 1 章 概述

## 数据结构简介

当面对数据结构时,我们通常会想到特定的行为或者操作,我们通常也希望对它们执行这些操作。例如,给定
一个链表,我们会自然地想到插入、移除、遍历和计算元素个数等操作。数据结构加上这些基本操作就称为
抽象数据类型(ADT)。一个抽象数据类型的操作就组成它的公共接口。抽象数据类型的公共接口精确地定义
了我们可以对它做什么。建立并遵守抽象数据类型的接口是绝对必要的,因为这会使我们能更好地管理程序的
数据,使得程序变得更容易理解也更容易维护。    

## 算法简介

从广义上讲,很多算法解决问题的思路是相同的。因此,为了方便,通常按照算法采用的方法和思路来给它们分类。
这样给算法分类的一个原因是:如果我们理解了它采用的一般思路我们常常就能够对该算法获得一些深入的了解。
在解决一些没有现成算法求解,但与现有问题类似的问题时,我们从中可以得到一些启发和灵感。当然,有些算法有
悖于分类原则,而另一些则是多种方法相结合的产物。这一节将介绍一些常用的方法。    

### 随机法

随机法依赖于随机数的统计特性。一个应用随机法的例子是快速排序。   

快速排序按如下方式工作:设想要对一堆作废的支票排序,首先将无序的一整堆分成两部分。其中一堆里使所有
的支票号码都小于等于所设定的一个中间值,另一堆里保证所有的支票号码都大于这个中间值。一旦有了这样
的两堆支票后,就再以同样的方式对这两堆支票重复刚才的划分过程,直到每一堆里都只有一张支票为止。这个
时候所有的支票就都排好序了。     

为了获得较高的性能,快速排序依赖于每一次要如何划分支票,我们需要让划分出的两堆支票数量几乎相同。
为了实现这一步,理想的方法是在划分支票之前首先找到支票号码的中间值。可是为了确定这个中间值需要遍
历所有的支票,因此我们并不打算这么做。作为替代的方法,随机选择一个支票号码作为划分的依据。快速排序的平均性能很不错,因为随机数的正态分布使得划分的结果是相对平衡的。     

话说快排只用到了随机法的理念吗，有没有下面分治法的概念呢。    

### 分治法

分治法包含3个步骤:分解、求解与合并。在分解阶段,将数据分解为更小、更容易管理的部分。在求解阶段,
对每个分解出的部分进行处理。在合并阶段,将每部分处理的结果进行合并。一个分治法的例子是归并排序。    

归并排序按照如下方式工作。如前所述,同样假设要排序一堆作废的支票。首先将无序的一整堆分成两半,
下一步分别将两堆支票再各自分成两半,一直持续这个步骤直到每一堆中都只有一张支票为止。一旦所有堆中
都只有一张支票时,就将其两两合并且保证每一个合并后的新堆都是有序的。一直做两两合并直到重新得到
一大堆,此时所有的支票就都已经排好序了。     

### 动态规划

动态规划同分治法类似,都是将较大的问题分解为子问题最后再将结果合并。然而,它们处理问题的方式与
子问题之间的关系有关。在分治法中,每一个子问题都是独立的。为此,我们以递归的方式解决每一个子问题,
然后将结果与其他子问题的结果合并。在动态规划中,子问题之间并不是独立的。换句话说,子问题之间可能
有关联。这类问题采用动态规划法比分治法更合适。因为若用分治法来解决这类问题会多做很多不必要的工作,
有些子问题会重复计算多次。    

### 贪心法

贪心法在求解问题时总能够做出在当前的最佳选择。换句话说,不是从整体最优上考虑,而仅仅是在某种意义
上的局部最优解。遗憾的是,当前的最优解长远来看却未必是最优的。因此,贪心法并不会一直产生最优结果。
然而,在某些方面来说,贪心法确是最佳选择。一个采用贪心法的例子是霍夫曼编码,这是一个数据压缩算法。   

### 近似法

近似法并不计算出最优解,相反,它只计算出“足够好”的解。通常利用近似法解决那些计算成本很高又因为
其本身十分有价值而不愿放弃的问题。推销员问题是一个通常会用近似法去解决的问题。    

设想一位推销员需要设计一条去往好几个城市工作的路线。推销员问题的目的是找到最短的可能路径,以便
推销员能够在回到出发点前恰好每座城市都只去过一次。由于推销员问题可能存在一种最优的策略,但计算
的代价很高,因此可以采用启发式的方法得到一个近似解。当最优策略行不通时,启发式的方法是我们能够
接受的一种比最优策略稍逊一些的策略。     

# 第 2 章 指针操作

当声明一个指针时,仅仅只是为指针本身分配了空间,并没有为指针所引用的数据分配空间。而为数据分配
存储空间有两种方法:一种是直接声明一个变量;另一种是在运行时动态地分配存储空间(例如:使用malloc
或realloc)。     

在C语言中,当想要动态分配存储空间时,我们会得到一个指向一个堆存储空间的指针。此存储空间由我们
自行管理,并且会一直存在,除非我们显式地将它释放。例如:在下面这段代码中用malloc分配的存储空间
会一直有效直到调用函数free来释放它。     

## 数组

为了理解C语言中指针与数组的关系,我做如下解释。我们知道要访问一个数组的第i个元素,用表达式:   

`a[i]`   

之所以此表达式能够访问a的第i个元素,是因为在C语言中,这个表达式与指向a的第一元素的指针表达意思
相同,也就是说,该表达式等同于以下表达式:   

`*(a+i)`    

此表达式实际上是使用指针运算的规则来访问元素的。简单来说,当对指针进行加一个整数i操作时,实际
得到了一个地址,这个地址由a所在的地址加上数据类型a所含字节数乘以i得到;而并不是简单地在a所
在的地址上加i个字节。当从指针中减去一个整数时也是执行类似的操作。这样我们也就解释了为什么数组的
索引是从0开始的,因为数组的第一个元素在位置0。    

# 第 3 章 递归

递归过程中的两个基本阶段:递推与回归。在递推阶段,每一个递归调用通过进一步调用自己来记住这次递归
过程。当其中有调用满足终止条件时,递推结束。比如,在计算n的阶乘时,终止条件是当n=1和n=0,此时函数
只须简单地返回1即可。每一个递归函数都必须拥有至少一个终止条件;否则,递推阶段就永远不会结束了。
一旦递推阶段结束,处理过程就进入回归阶段,在这之前的函数调用以逆序的方式回归,直到最初调用的函数
返回为止,此时递归过程结束。    

如果一个函数中所有递归形式的调用都出现在函数的末尾,我们称这个递归函数是尾递归的。当递归调用是
整个函数体中最后执行的语句且它的返回值不属于表达式的一部分时,这个递归调用就是尾递归。尾递归
函数的特点是在回归过程中不用做任何操作,这个特性很重要,因为大多数现代的编译器会利用这种特点
自动生成优化的代码。    

当编译器检测到一个函数调用是尾递归的时候,它就覆盖当前的活跃记录而不是在栈中去创建一个新的。
编译器可以做到这点,因为递归调用是当前活跃期内最后一条待执行的语句,于是当这个调用返回时栈
帧中并没有其他事情可做,因此也就没有保存栈帧的必要了。通过覆盖当前的栈帧而不是在其之上重新添加
一个,这样所使用的栈空间就大大缩减了,这使得实际的运行效率会变得更高。因此,只要有可能我们就
需要将递归函数写成尾递归的形式。    

# 第 4 章 算法分析

最坏情况可以告诉我们算法性能的上限。分析一个算法的最坏情况可以保证在任何情况下此算法的表现都
不会比最坏情况差,而其他情况肯定比最坏情况要好。    

虽然我们把最坏情况当做很多算法性能的度量,但也有例外。因为有些时候我们也会用平均情况来评判
算法性能。例如:随机算法中的快速排序算法,它使用了概率论的理论基础,从而有效地保证了平均情况下
性能的准确性。    

常见复杂计算发生的复杂度：   


复杂度 | 例子
----------|---------
 O(1) | 从一个数据集中获取第一个元素
 O(lgn) | 将一个数据集分成两半，然后将分开的每一半再分成两半，以此类推
 O(n) | 遍历一个数据集
 O(nlgn) | 将一个数据集分成两半，然后将分开的每一半再分成两半，依次类推，在此过程中同时遍历每一半数据
 O(n^2) | 遍历一个数据集中每个元素的同时同时遍历另一个数量级相同的数据集
 O(2^n) | 为一个数据集生成其可能的所有子集
 O(n!) | 为一个数据集生成其可能的所有排列组合    

就像一种算法的复杂度几乎不关注算法具体的运行时间,衡量算法的复杂度也没有高效或低效之说。虽然复杂度
在一定程度上说明了算法的运算效率,但一个特定的复杂度要根据具体的情况来衡量它是否高效。一般来说,
在给定一定标准的情况下,能够使此算法表现最佳时,我们就认为此算法是高效的。通常,在解决同一个问题时,
如果一种算法的复杂度比其他算法的复杂度都低,并且没有过多的常数项,我们就可以认为此算法是高效的。
但也会有一些棘手的问题,在这些问题中,如果不设定一个近似值就无法找到一个“有效的”解决方法。这是一类特
殊的问题,称为NP完全问题。    

# 第 5 章 链表

**双向链表**：这种形式的链表元素之间通过两个指针链接。双向链表可以正向遍历，也可以反向遍历。   

**循环链表**：这种形式的链表最后一个元素指针指向链表的首元素，而不是设置为 NULL。这种结构
的链表允许循环遍历。   

一些链表的应用包括：   

+ 邮件列表
+ 多项式计算
+ 内存管理
+ 文件的链式分配
+ 其他的数据结构

# 第 7 章 集合

集合是不同对象(称为成员)的无序聚集。由于元素之间彼此相关联,因此可以理解为归聚在一起的成员组合。
集合的两个重要特点是:第一,成员是无序的,第二,每个成员都只在集合中出现一次。     

集合接口该有的一些操作：   

+ `insert(value) int`: 如果插入成功返回 0，如果插入的成员在集合中已经存在返回 1，否则返回 -1。复杂度 O(n)
+ `remove(value) int`: 移除成功返回 0，否则返回 -1.复杂度 O(n)
+ `union(set2) int`: 并集，成功返回0，否则返回 -1.复杂度 O(mn)
+ `intersection(set2) int`: 交集，成功返回 0，否则返回 -1.复杂度 O(mn)
+ `difference(set2) int`: 差集，成功返回 0，否则返回 -1.复杂度 O(mn)
+ `contains(value) int`: 找到成员返回 1.否则返回 -1. O(n)
+ `isSubset(set2) int`: 如果 set 是 set2 的子集返回 1，否则返回 -1. O(mn)
+ `isEqual(set2) int`: 相等返回 1，否则返回 -1. O(mn)    

这里先不讲如何实现，只讲一下实现的具体思路：   

+ insert: 首选要使用 contains 判断一下成员是否已经在集合中了，然后才可以安全插入，contains 复杂度 O(n)，链表在头部插入 O(1)，整体复杂度 O(n)
+ remove: 直接遍历就好了，保存好 prev 指针，方便找到后删除
+ union: 先把第一个集合元素全部推入，然后把第二个中不在第一个中的成员推入，复杂度 O(n) + mO(n) = O(mn)
+ intersection: 遍历第一个集合，然后检查每个成员是否在第二个集合中
+ difference: 遍历第一个集合，然后检查每个成员是否在第二个集合中
+ contains: 遍历整个集合
+ isSubset: 集合一的元素是否全部在集合二中可以找到
+ isEqual: 成员个数相同，并且集合一是集合二的子集


# 第 8 章 哈希表

哈希表支持一种最有效的检索方法:散列。从根本上来说,一个哈希表包含一个数组,通过特殊的索引值(键)
来访问数组中的元素。哈希表的主要思想是通过一个哈希函数,在所有可能的键与槽位之间建立一张映射表。
哈希函数每次接受一个键将返回与键相对应的哈希编码或哈希值。键的数据类型可能多种多样,但哈希值的
类型只能是整型。    

由于计算哈希值和在数组中进行索引都只消耗固定的时间,因此哈希表的最大亮点在于它是一种运行时间在
常量级的检索方法。当哈希函数能够保证不同的键生成的哈希值互不相同时,就说哈希表能直接寻址想要的结果。
但这只是理想状态,在实际运用过程中,能够直接寻址结果的情况非常少。因为这样的话数组会非常大，而且
也很难确保哈希值都是一对一的。   

通常与各种各样的键相比,哈希表的条目数相应较少。因此,绝大多数哈希函数会将一些不同的键映射到表中
相同的槽位上。当两个键映射到一个相同的槽位上时,它们就产生了冲突。一个好的哈希函数能最大限度地
减少冲突,但冲突不可能完全消除,我们仍然要想办法处理这些冲突。    

## 链式哈希表

链式哈希表从根本上来说是由一组链表构成。每个链表都可以看做一个“桶”,我们将所有的元素通过散列的
方式放到具体的不同的桶中插入元素时,首先将其键传入一个哈希函数(该过程称为哈希键),函数通过散列的
方式告知元素属于哪个“桶”,然后在相应的链表头插入元素。查找或删除元素时,用同样的方式先找到元素
的“桶”,然后遍历相应的链表,直到发现我们想要查找的元素。因为每个“桶”都是一个链表,所以链式哈希表
并不限制包含元素的个数。然而,如果表变得太大,它的性能将会降低。    

### 解决冲突

当哈希表中两个键散列到一个相同的槽位时,这两个键之间将会产生冲突。链式哈希表解决冲突的方法非常简单:
当冲突发生时,它就将元素放到已经准备好的“桶”中。但这同样会带来一个问题,当过多的冲突发生在同一槽位时,
此位置的“桶”将会变得越来越深,从而造成访问这个位置的元素所需要的时间越来越多。   

在理想情况下,我们希望所有“桶”以几乎同样的速度增长,这样它们就可以尽可能地保持小的容量和相同的大小。
换句话说,我们的目标就是尽可能地均匀和随机地分配表中的元素,这种情况在理论上称为均匀散列,而在实际中,
我们只能尽可能近似达到这种状态。    

如果想插入表中的元素数量远大于表中“桶”的数量,那么即使是在一个均匀散列的过程中,表的性能也会迅速降低。
在这种情况下,表中所有的“桶”都变得越来越深。因此,我们必须要特别注意一个哈希表的负载因子,其定义为:   

α=n/m    

其中,n是表中元素的个数,m是“桶”的个数。在均匀散列的情况下,链式哈希表的负载因子告诉我们表中的“桶”
能装下元素个数的最大值。    

### 选择哈希函数

一个好的哈希函数旨在近似均匀散列,也就是,尽可能以均匀和随机的方式散布一个哈希表中的元素。定义
一个哈希函数f,它将键k映射到哈希表中的位置x。x称为k的哈希编码,正式的表述为:   

h(k) = x    

一般来说,大多数的散列方法都假设k为整数,这样k能够很容易地以数学方式修改,从而使得h能够更均匀地
将元素分布在表中。当k不是一个整数时,我们也可以很容易地将它强制转换为整型。    

1. **取余法**    

有一个整型键k,一种最简单地将k映射到m槽位的散列方法是计算k除以m的所得到的余数。我们称为取余法,
正式的表述为: h(k) = k mod m    

如果表有m=1699个位置,而要散列的键 k=25657,通过这种方法可以得到哈希编码为 25657 mod 1699=172。
通常情况下,要避免 m 的值为 2 的幂。这是因为假如 m=2^p ,那么 h 仅仅是 k 的 p 个最低阶位。
通常我们选择的 m 会是一个素数,且不要太接近于2的幂,同时还要考虑存储空间的限制和负载因子。    

2. **乘法**    

与取余法不同的是乘法,它将整型键 k 乘以一个常数A(0<A<1);取结果的小数部分;然后乘以 m 取结果的
整数部分。通常情况下, A取0.618,它由根号 5 减1再除以2得到。这个方法称为乘法。    

这种方法有个优点是,对于表中槽位个数m的选择并不需要像取余法中那么慎重。同时请注意,这个散列方法
可以让我们更灵活地选择m,以便获取我们可以接受的最大“桶”深。    

### 链式哈希表的操作定义

+ init: O(m)，m 为桶的个数，这时因为 O(1) 的 list init 操作要执行 m 次
+ insert: O(1)，首先计算 hash 索引，O(1)，在链表头插入 O(1)
+ remove: O(1)，这里书上说是 O(1)，是因为通常我们需要在某个桶中遍历该链表，而这个桶能容量的
元素个数通常是一个较小的常量。
+ loolup: O(1),同上

## 开地址哈希表



# 第 9 章 树

## 树的平衡

树的平衡是指对于给定数量的结点，保证树的高度尽可能短的过程。这意味着在结点加入下一层之前
必须保证本层结点满额。正式的说法是，如果满足树的所有叶子结点都在同一层上，或者所有叶子结点
都在最后两层上，且倒数第二层是满的，则这棵树是平衡的。如果一颗平衡树最后一层的所有叶子结点
都在最靠左边的位置上，则称这棵树是左平衡的。