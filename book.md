<!-- TOC -->

- [第 1 章 概述](#第-1-章-概述)
  - [数据结构简介](#数据结构简介)
  - [算法简介](#算法简介)
    - [随机法](#随机法)
    - [分治法](#分治法)
    - [动态规划](#动态规划)
    - [贪心法](#贪心法)
    - [近似法](#近似法)
- [第 2 章 指针操作](#第-2-章-指针操作)
  - [数组](#数组)
- [第 3 章 递归](#第-3-章-递归)
- [第 4 章 算法分析](#第-4-章-算法分析)
- [第 5 章 链表](#第-5-章-链表)
- [第 7 章 集合](#第-7-章-集合)
- [第 8 章 哈希表](#第-8-章-哈希表)
  - [链式哈希表](#链式哈希表)
    - [解决冲突](#解决冲突)
    - [选择哈希函数](#选择哈希函数)
    - [链式哈希表的操作定义](#链式哈希表的操作定义)
  - [开地址哈希表](#开地址哈希表)
    - [线性探查](#线性探查)
    - [双散列](#双散列)
    - [开地址哈希函数的接口定义](#开地址哈希函数的接口定义)
- [第 9 章 树](#第-9-章-树)
  - [树的平衡](#树的平衡)
  - [AVL 树](#avl-树)
- [第 10 章 图](#第-10-章-图)
  - [广度优先搜索](#广度优先搜索)
  - [深度优先搜索](#深度优先搜索)
- [第 12 章 排序和搜索](#第-12-章-排序和搜索)
  - [插入排序](#插入排序)
  - [快排](#快排)
  - [归并排序](#归并排序)
  - [计数排序](#计数排序)
  - [基数排序](#基数排序)
  - [问与答](#问与答)
- [第 14 章 数据压缩](#第-14-章-数据压缩)
  - [位操作](#位操作)
    - [bit get](#bit-get)
    - [bit set](#bit-set)

<!-- /TOC -->
# 第 1 章 概述

## 数据结构简介

当面对数据结构时,我们通常会想到特定的行为或者操作,我们通常也希望对它们执行这些操作。例如,给定
一个链表,我们会自然地想到插入、移除、遍历和计算元素个数等操作。数据结构加上这些基本操作就称为
抽象数据类型(ADT)。一个抽象数据类型的操作就组成它的公共接口。抽象数据类型的公共接口精确地定义
了我们可以对它做什么。建立并遵守抽象数据类型的接口是绝对必要的,因为这会使我们能更好地管理程序的
数据,使得程序变得更容易理解也更容易维护。    

## 算法简介

从广义上讲,很多算法解决问题的思路是相同的。因此,为了方便,通常按照算法采用的方法和思路来给它们分类。
这样给算法分类的一个原因是:如果我们理解了它采用的一般思路我们常常就能够对该算法获得一些深入的了解。
在解决一些没有现成算法求解,但与现有问题类似的问题时,我们从中可以得到一些启发和灵感。当然,有些算法有
悖于分类原则,而另一些则是多种方法相结合的产物。这一节将介绍一些常用的方法。    

### 随机法

随机法依赖于随机数的统计特性。一个应用随机法的例子是快速排序。   

快速排序按如下方式工作:设想要对一堆作废的支票排序,首先将无序的一整堆分成两部分。其中一堆里使所有
的支票号码都小于等于所设定的一个中间值,另一堆里保证所有的支票号码都大于这个中间值。一旦有了这样
的两堆支票后,就再以同样的方式对这两堆支票重复刚才的划分过程,直到每一堆里都只有一张支票为止。这个
时候所有的支票就都排好序了。     

为了获得较高的性能,快速排序依赖于每一次要如何划分支票,我们需要让划分出的两堆支票数量几乎相同。
为了实现这一步,理想的方法是在划分支票之前首先找到支票号码的中间值。可是为了确定这个中间值需要遍
历所有的支票,因此我们并不打算这么做。作为替代的方法,随机选择一个支票号码作为划分的依据。快速排序的平均性能很不错,因为随机数的正态分布使得划分的结果是相对平衡的。     

话说快排只用到了随机法的理念吗，有没有下面分治法的概念呢。    

### 分治法

分治法包含3个步骤:分解、求解与合并。在分解阶段,将数据分解为更小、更容易管理的部分。在求解阶段,
对每个分解出的部分进行处理。在合并阶段,将每部分处理的结果进行合并。一个分治法的例子是归并排序。    

归并排序按照如下方式工作。如前所述,同样假设要排序一堆作废的支票。首先将无序的一整堆分成两半,
下一步分别将两堆支票再各自分成两半,一直持续这个步骤直到每一堆中都只有一张支票为止。一旦所有堆中
都只有一张支票时,就将其两两合并且保证每一个合并后的新堆都是有序的。一直做两两合并直到重新得到
一大堆,此时所有的支票就都已经排好序了。     

### 动态规划

动态规划同分治法类似,都是将较大的问题分解为子问题最后再将结果合并。然而,它们处理问题的方式与
子问题之间的关系有关。在分治法中,每一个子问题都是独立的。为此,我们以递归的方式解决每一个子问题,
然后将结果与其他子问题的结果合并。在动态规划中,子问题之间并不是独立的。换句话说,子问题之间可能
有关联。这类问题采用动态规划法比分治法更合适。因为若用分治法来解决这类问题会多做很多不必要的工作,
有些子问题会重复计算多次。    

### 贪心法

贪心法在求解问题时总能够做出在当前的最佳选择。换句话说,不是从整体最优上考虑,而仅仅是在某种意义
上的局部最优解。遗憾的是,当前的最优解长远来看却未必是最优的。因此,贪心法并不会一直产生最优结果。
然而,在某些方面来说,贪心法确是最佳选择。一个采用贪心法的例子是霍夫曼编码,这是一个数据压缩算法。   

### 近似法

近似法并不计算出最优解,相反,它只计算出“足够好”的解。通常利用近似法解决那些计算成本很高又因为
其本身十分有价值而不愿放弃的问题。推销员问题是一个通常会用近似法去解决的问题。    

设想一位推销员需要设计一条去往好几个城市工作的路线。推销员问题的目的是找到最短的可能路径,以便
推销员能够在回到出发点前恰好每座城市都只去过一次。由于推销员问题可能存在一种最优的策略,但计算
的代价很高,因此可以采用启发式的方法得到一个近似解。当最优策略行不通时,启发式的方法是我们能够
接受的一种比最优策略稍逊一些的策略。     

# 第 2 章 指针操作

当声明一个指针时,仅仅只是为指针本身分配了空间,并没有为指针所引用的数据分配空间。而为数据分配
存储空间有两种方法:一种是直接声明一个变量;另一种是在运行时动态地分配存储空间(例如:使用malloc
或realloc)。     

在C语言中,当想要动态分配存储空间时,我们会得到一个指向一个堆存储空间的指针。此存储空间由我们
自行管理,并且会一直存在,除非我们显式地将它释放。例如:在下面这段代码中用malloc分配的存储空间
会一直有效直到调用函数free来释放它。     

## 数组

为了理解C语言中指针与数组的关系,我做如下解释。我们知道要访问一个数组的第i个元素,用表达式:   

`a[i]`   

之所以此表达式能够访问a的第i个元素,是因为在C语言中,这个表达式与指向a的第一元素的指针表达意思
相同,也就是说,该表达式等同于以下表达式:   

`*(a+i)`    

此表达式实际上是使用指针运算的规则来访问元素的。简单来说,当对指针进行加一个整数i操作时,实际
得到了一个地址,这个地址由a所在的地址加上数据类型a所含字节数乘以i得到;而并不是简单地在a所
在的地址上加i个字节。当从指针中减去一个整数时也是执行类似的操作。这样我们也就解释了为什么数组的
索引是从0开始的,因为数组的第一个元素在位置0。    

# 第 3 章 递归

递归过程中的两个基本阶段:递推与回归。在递推阶段,每一个递归调用通过进一步调用自己来记住这次递归
过程。当其中有调用满足终止条件时,递推结束。比如,在计算n的阶乘时,终止条件是当n=1和n=0,此时函数
只须简单地返回1即可。每一个递归函数都必须拥有至少一个终止条件;否则,递推阶段就永远不会结束了。
一旦递推阶段结束,处理过程就进入回归阶段,在这之前的函数调用以逆序的方式回归,直到最初调用的函数
返回为止,此时递归过程结束。    

如果一个函数中所有递归形式的调用都出现在函数的末尾,我们称这个递归函数是尾递归的。当递归调用是
整个函数体中最后执行的语句且它的返回值不属于表达式的一部分时,这个递归调用就是尾递归。尾递归
函数的特点是在回归过程中不用做任何操作,这个特性很重要,因为大多数现代的编译器会利用这种特点
自动生成优化的代码。    

当编译器检测到一个函数调用是尾递归的时候,它就覆盖当前的活跃记录而不是在栈中去创建一个新的。
编译器可以做到这点,因为递归调用是当前活跃期内最后一条待执行的语句,于是当这个调用返回时栈
帧中并没有其他事情可做,因此也就没有保存栈帧的必要了。通过覆盖当前的栈帧而不是在其之上重新添加
一个,这样所使用的栈空间就大大缩减了,这使得实际的运行效率会变得更高。因此,只要有可能我们就
需要将递归函数写成尾递归的形式。    

# 第 4 章 算法分析

最坏情况可以告诉我们算法性能的上限。分析一个算法的最坏情况可以保证在任何情况下此算法的表现都
不会比最坏情况差,而其他情况肯定比最坏情况要好。    

虽然我们把最坏情况当做很多算法性能的度量,但也有例外。因为有些时候我们也会用平均情况来评判
算法性能。例如:随机算法中的快速排序算法,它使用了概率论的理论基础,从而有效地保证了平均情况下
性能的准确性。    

常见复杂计算发生的复杂度：   


复杂度 | 例子
----------|---------
 O(1) | 从一个数据集中获取第一个元素
 O(lgn) | 将一个数据集分成两半，然后将分开的每一半再分成两半，以此类推
 O(n) | 遍历一个数据集
 O(nlgn) | 将一个数据集分成两半，然后将分开的每一半再分成两半，依次类推，在此过程中同时遍历每一半数据
 O(n^2) | 遍历一个数据集中每个元素的同时同时遍历另一个数量级相同的数据集
 O(2^n) | 为一个数据集生成其可能的所有子集
 O(n!) | 为一个数据集生成其可能的所有排列组合    

就像一种算法的复杂度几乎不关注算法具体的运行时间,衡量算法的复杂度也没有高效或低效之说。虽然复杂度
在一定程度上说明了算法的运算效率,但一个特定的复杂度要根据具体的情况来衡量它是否高效。一般来说,
在给定一定标准的情况下,能够使此算法表现最佳时,我们就认为此算法是高效的。通常,在解决同一个问题时,
如果一种算法的复杂度比其他算法的复杂度都低,并且没有过多的常数项,我们就可以认为此算法是高效的。
但也会有一些棘手的问题,在这些问题中,如果不设定一个近似值就无法找到一个“有效的”解决方法。这是一类特
殊的问题,称为NP完全问题。    

# 第 5 章 链表

**双向链表**：这种形式的链表元素之间通过两个指针链接。双向链表可以正向遍历，也可以反向遍历。   

**循环链表**：这种形式的链表最后一个元素指针指向链表的首元素，而不是设置为 NULL。这种结构
的链表允许循环遍历。   

一些链表的应用包括：   

+ 邮件列表
+ 多项式计算
+ 内存管理
+ 文件的链式分配
+ 其他的数据结构

# 第 7 章 集合

集合是不同对象(称为成员)的无序聚集。由于元素之间彼此相关联,因此可以理解为归聚在一起的成员组合。
集合的两个重要特点是:第一,成员是无序的,第二,每个成员都只在集合中出现一次。     

集合接口该有的一些操作：   

+ `insert(value) int`: 如果插入成功返回 0，如果插入的成员在集合中已经存在返回 1，否则返回 -1。复杂度 O(n)
+ `remove(value) int`: 移除成功返回 0，否则返回 -1.复杂度 O(n)
+ `union(set2) int`: 并集，成功返回0，否则返回 -1.复杂度 O(mn)
+ `intersection(set2) int`: 交集，成功返回 0，否则返回 -1.复杂度 O(mn)
+ `difference(set2) int`: 差集，成功返回 0，否则返回 -1.复杂度 O(mn)
+ `contains(value) int`: 找到成员返回 1.否则返回 -1. O(n)
+ `isSubset(set2) int`: 如果 set 是 set2 的子集返回 1，否则返回 -1. O(mn)
+ `isEqual(set2) int`: 相等返回 1，否则返回 -1. O(mn)    

这里先不讲如何实现，只讲一下实现的具体思路：   

+ insert: 首选要使用 contains 判断一下成员是否已经在集合中了，然后才可以安全插入，contains 复杂度 O(n)，链表在头部插入 O(1)，整体复杂度 O(n)
+ remove: 直接遍历就好了，保存好 prev 指针，方便找到后删除
+ union: 先把第一个集合元素全部推入，然后把第二个中不在第一个中的成员推入，复杂度 O(n) + mO(n) = O(mn)
+ intersection: 遍历第一个集合，然后检查每个成员是否在第二个集合中
+ difference: 遍历第一个集合，然后检查每个成员是否在第二个集合中
+ contains: 遍历整个集合
+ isSubset: 集合一的元素是否全部在集合二中可以找到
+ isEqual: 成员个数相同，并且集合一是集合二的子集


# 第 8 章 哈希表

哈希表支持一种最有效的检索方法:散列。从根本上来说,一个哈希表包含一个数组,通过特殊的索引值(键)
来访问数组中的元素。哈希表的主要思想是通过一个哈希函数,在所有可能的键与槽位之间建立一张映射表。
哈希函数每次接受一个键将返回与键相对应的哈希编码或哈希值。键的数据类型可能多种多样,但哈希值的
类型只能是整型。    

由于计算哈希值和在数组中进行索引都只消耗固定的时间,因此哈希表的最大亮点在于它是一种运行时间在
常量级的检索方法。当哈希函数能够保证不同的键生成的哈希值互不相同时,就说哈希表能直接寻址想要的结果。
但这只是理想状态,在实际运用过程中,能够直接寻址结果的情况非常少。因为这样的话数组会非常大，而且
也很难确保哈希值都是一对一的。   

通常与各种各样的键相比,哈希表的条目数相应较少。因此,绝大多数哈希函数会将一些不同的键映射到表中
相同的槽位上。当两个键映射到一个相同的槽位上时,它们就产生了冲突。一个好的哈希函数能最大限度地
减少冲突,但冲突不可能完全消除,我们仍然要想办法处理这些冲突。    

## 链式哈希表

链式哈希表从根本上来说是由一组链表构成。每个链表都可以看做一个“桶”,我们将所有的元素通过散列的
方式放到具体的不同的桶中插入元素时,首先将其键传入一个哈希函数(该过程称为哈希键),函数通过散列的
方式告知元素属于哪个“桶”,然后在相应的链表头插入元素。查找或删除元素时,用同样的方式先找到元素
的“桶”,然后遍历相应的链表,直到发现我们想要查找的元素。因为每个“桶”都是一个链表,所以链式哈希表
并不限制包含元素的个数。然而,如果表变得太大,它的性能将会降低。    

### 解决冲突

当哈希表中两个键散列到一个相同的槽位时,这两个键之间将会产生冲突。链式哈希表解决冲突的方法非常简单:
当冲突发生时,它就将元素放到已经准备好的“桶”中。但这同样会带来一个问题,当过多的冲突发生在同一槽位时,
此位置的“桶”将会变得越来越深,从而造成访问这个位置的元素所需要的时间越来越多。   

在理想情况下,我们希望所有“桶”以几乎同样的速度增长,这样它们就可以尽可能地保持小的容量和相同的大小。
换句话说,我们的目标就是尽可能地均匀和随机地分配表中的元素,这种情况在理论上称为均匀散列,而在实际中,
我们只能尽可能近似达到这种状态。    

如果想插入表中的元素数量远大于表中“桶”的数量,那么即使是在一个均匀散列的过程中,表的性能也会迅速降低。
在这种情况下,表中所有的“桶”都变得越来越深。因此,我们必须要特别注意一个哈希表的负载因子,其定义为:   

α=n/m    

其中,n是表中元素的个数,m是“桶”的个数。在均匀散列的情况下,链式哈希表的负载因子告诉我们表中的“桶”
能装下元素个数的最大值。    

### 选择哈希函数

一个好的哈希函数旨在近似均匀散列,也就是,尽可能以均匀和随机的方式散布一个哈希表中的元素。定义
一个哈希函数f,它将键k映射到哈希表中的位置x。x称为k的哈希编码,正式的表述为:   

h(k) = x    

一般来说,大多数的散列方法都假设k为整数,这样k能够很容易地以数学方式修改,从而使得h能够更均匀地
将元素分布在表中。当k不是一个整数时,我们也可以很容易地将它强制转换为整型。    

1. **取余法**    

有一个整型键k,一种最简单地将k映射到m槽位的散列方法是计算k除以m的所得到的余数。我们称为取余法,
正式的表述为: h(k) = k mod m    

如果表有m=1699个位置,而要散列的键 k=25657,通过这种方法可以得到哈希编码为 25657 mod 1699=172。
通常情况下,要避免 m 的值为 2 的幂。这是因为假如 m=2^p ,那么 h 仅仅是 k 的 p 个最低阶位。
通常我们选择的 m 会是一个素数,且不要太接近于2的幂,同时还要考虑存储空间的限制和负载因子。    

2. **乘法**    

与取余法不同的是乘法,它将整型键 k 乘以一个常数A(0<A<1);取结果的小数部分;然后乘以 m 取结果的
整数部分。通常情况下, A取0.618,它由根号 5 减1再除以2得到。这个方法称为乘法。    

这种方法有个优点是,对于表中槽位个数m的选择并不需要像取余法中那么慎重。同时请注意,这个散列方法
可以让我们更灵活地选择m,以便获取我们可以接受的最大“桶”深。    

### 链式哈希表的操作定义

+ init: O(m)，m 为桶的个数，这时因为 O(1) 的 list init 操作要执行 m 次
+ insert: O(1)，首先计算 hash 索引，O(1)，在链表头插入 O(1)
+ remove: O(1)，这里书上说是 O(1)，是因为通常我们需要在某个桶中遍历该链表，而这个桶能容量的
元素个数通常是一个较小的常量。
+ loolup: O(1),同上

## 开地址哈希表

在链式哈希表中，元素存放在每个地址的“桶”中。而在开地址哈希表中，元素存放在表本身中。这种
特性对于某些依赖于固定大小表的应用来说非常有用。然而，因为在每个槽位上没有一个“桶”来存储
冲突的元素，所以开地址哈希表需要通过另一种方法来解决冲突。    

在开地址哈希表中解决冲突的方法就是探查这个表，直到找到一个可以放置元素的槽。例如，如果要插
入一个元素，我们探查槽位直到找到一个空槽，然后将元素插入此槽中。如果要删除或查找一个元素，
我们探查槽位直到定位到该元素或直到找到一个空槽。如果在找到元素之前找到一个空槽或遍历完所有
槽位，那么说明此元素在表中不存在。   

当然，在进行操作时要尽可能地减少探查的次数。究竟进行过多少次探查后就停止探查主要取决于两件事：
哈希表的负载因子和元素均匀分布的程度。回想一下，哈希表的负载因子α=n/m，其中n为元素的个数，
m为可以散列元素的槽位个数。要注意，根据开地址哈希表的定义，它所包含的元素不可能大于表中
槽位的数量（n＞m），所以开地址哈希表的负载因子通常小于或等于1。这是显而易见的，因为每个
槽至多能够容纳一个元素。    

### 线性探查

开地址哈希表中一种简单的探查方法就是探查表中连续的槽位。正式地表述为，如果i大于0小于m-1
（m为表中的槽位个数），那么一个线性探查方法的哈希函数定义为：   

h(k, i) = (h'(k) + i) mod m    

其中，k是键，i是到目前为止探查的次数。函数h'是一个辅助哈希函数，就像任何哈希函数的选择方法一
样，它会尽可能地将元素随机和均匀地分布在表中。例如，可以采用取余法，这样h'(k)=k mod m。
在这种情况下，如果将一个元素（键k=2998）散列到表（容量m=1000），所得到的哈希编码为
(998+0) mod 1000=998（当i=0时），(998+1) mod 1000=999（当i=1时），(998+2) mod 1000=0
（当i=2时），依此类推。所以，当要插入一个键k=2998的元素时，我们会寻找一个空的槽位，首先探查
槽位998，然后槽位999，然后槽位0，依此类推。    

### 双散列

最有效地探查开地址哈希表的方法之一，就是通过计算两个辅助哈希函数哈希编码的和来得到哈希编码。
正式地表述为，如果i的大小在0和m之间（m为表中槽位个数），双散列的哈希函数定义为：   

h(k, i) = (h1(k) + ih2(k)) mod m    

### 开地址哈希函数的接口定义

+ init: O(m) 没看懂，解释不了，书上说这是由于m个指向槽中数据的指针都必须初始化为NULL
+ insert: O(1) 这是因为，为了找到一个能够插入元素的空槽，我们可能会探查1/（1-α）个槽，
这是一个很小的常量。
+ remove: O(1) 同理
+ lookuo: O(1) 同理   

# 第 9 章 树

关于树的一些应用包括：   

+ 霍夫曼编码，使用一颗霍夫曼树来压缩一组数据
+ 用户界面，窗口按照层次结构组织成一棵树
+ 数据库系统，B树
+ 人工智能
+ 优先级队列，采用一颗二叉树来记录集合中的哪个元素将拥有下一个最高的优先级

## 树的平衡

树的平衡是指对于给定数量的结点，保证树的高度尽可能短的过程。这意味着在结点加入下一层之前
必须保证本层结点满额。正式的说法是，如果满足树的所有叶子结点都在同一层上，或者所有叶子结点
都在最后两层上，且倒数第二层是满的，则这棵树是平衡的。如果一颗平衡树最后一层的所有叶子结点
都在最靠左边的位置上，则称这棵树是左平衡的。 

## AVL 树

要保持一棵二叉搜索树的平衡,实际做起来比看上去要难得多。尽管如此,还是有一些非常明智的办法可以采
用。其中最好的方法就是将二叉搜索树实现为AVL树。    

AVL树(Adel'son-Vel'skii and Landis)是一种特殊类型的二叉树,它的每个结点都保存一份额外的
信息:结点的平衡因子。结点的平衡因子是它的右子树高度减去它的左子树高度的结果。当插入结点时,
AVL树需要自我调整,使得所有结点的平衡因子保持为+1、-1或 0。当子树的根结点平衡因子为+1时,
它是左倾斜的(left-heavy)。当子树的根结点平衡因子为-1时,它是右倾斜的(right-heavy)。一棵子
树的根结点的平衡因子就代表该子树的平衡性。保持所有子树几乎都处
于平衡状态,AVL树在总体上就能够基本保持平衡。     

# 第 10 章 图

图由两种类型的元素组成:顶点和边。顶点代表对象,边则建立起对象之间的关系或关联。    

图的正式表示法是 G = (V, E), 这里 V 代表顶点的集合，而 E 和 V 之间是一种二元关系。在有向图
中，如果某条边是从顶点 u 开始到顶点 v 结束，则 E 包含有序对 (u, v)。在无向图中,由于边
(u,v)和(v,u)是一样的意思,因此在E中只需要记录其中一个就可以了。在有向图中边可能会指回同一个
顶点,但在无向图中则不会出现这种情况。   

图中的两个重要关系是邻接(adjacency)和关联(incidence)。邻接是两个顶点之间的一种关系。
如果图包含边(u,v),则称顶点 v 与顶点 u 邻接。在无向图中,这也暗示了顶点 u 也与顶点 v 邻接。
换句话说,在无向图中邻接关系是对称的。而在有向图中则并非如此。如果一幅图中的每一个顶点都与其他顶
点相邻接,则称这幅图是完全图。     

关联是指顶点和边之间的关系。在有向图中,边(u,v)从顶点 u 开始关联到 v,或者相反,从顶点 v 开始
关联到 u 。在有向图中,顶点的入度(in-degree)指的是以该
顶点为终点的边的数目。而顶点的出度(out-degree)指的是以该顶点为起点的边的数目。在无向
图中,边(u,v)与顶点 u 和 v 相关联,而顶点的度就是与该顶点相关联的边的数目。    

我们常常在图中说到路径。路径是依次遍历顶点序列之间的边所形成的轨迹。正式的说法是，顶点 u
到另一个顶点 u' 的路径由顶点序列 &lt;V<sub>0</sub>, V<sub>1</sub>, ..., V<sub>k</sub>
&gt; 组成，使用 u = V<sub>0</sub> 且 u' = V<sub>k</sub>，对于 i = 1, 2, ....., k，
所有的 (V<sub>i-1</sub>, V<sub>i</sub>) 均属于 E。这样的一条路径包含便 (V<sub>0</sub>,
V<sub>1</sub>), (V<sub>1</sub>, V<sub>2</sub>),....,(V<sub>k-1</sub>, V<sub>k</sub>)，
且长度为 k。如果存在一条从 u 到 u' 的路径，则 u' 从 u 是可达的。没有重复顶点的路径称为简单
路径。    

环是指路径包含相同的顶点两次或两次以上。也就是说,在有向图的一条路径中,如果从某顶点出发,最后
能够返回该顶点,则该路径是环。没有环的图称为无环图。有向无环图有特殊的名称,叫做DAG
(Directed Acyline Graph的缩写)。     

连通性是图中另一个重要的概念。对于无向图而言,如果它的每个顶点都能通过某条路径到达其他
顶点,那么我们称它为连通的。如果该条件在有向图中同样成立,则称该图是强连通的。尽管无向
图可能不是连通的,但它仍然可能包含连通的部分,这部分称为连通分支。如果有向图中只有部分
是强连通的,则该部分称为强连通分支。    

在计算机中最常用来表示图的方法是采用邻接表表示形式,邻接表按照链表的方式组织起来。链表中
的每个结构都包含两个成员:一个顶点和与该顶点邻接的顶点所组成的一个邻接表。    

@TODO 邻接表结构。    

注意邻接表好像是指右边的那些东西。    

在图G=(V,E)中,如果V中的两个顶点u和v组成E中的边(u,v),则顶点 v 包含在顶点 u 的邻接表中。
因而,在有向图中,所有邻接表中的顶点总数同总的边数相等。在无向图中,由于边(u,v)暗含了边
(v,u),因此顶点 v 包含在顶点 u 的邻接表中,而顶点 u 也包含在顶点 v 的邻接表中。因而,
在这种情况下所有邻接表中的顶点总数是总边数的两倍。    

通常,邻接表多用于稀疏图中,稀疏图是指边数相对来说较少的图。稀疏图非常普遍,但是如果图是稠密
型的,就应该选择采用邻接矩阵表示方式来表示稠密图了。    

## 广度优先搜索

广度优先搜索在进一步探索图中的顶点之前先访问当前顶点的所有邻接结点。这种查找方法在很多应用
中都非常有用,包括找出最小生成树以及最短路径问题。    

开始前,首先选择一个起始顶点并将其涂黑,而其他顶点为白色。把起始顶点单独置于一个队列中。该
算法按照如下方式处理:对于队列中的每个顶点(初始状态下只有起始顶点),从队列首部选出这个顶点
并找出每一个与之相邻接的顶点。将找到的邻接顶点入队到队列末尾。我们将已经访问过的顶点涂黑,
而还没有访问的顶点则是白色。如果顶点的颜色是灰色,表示已经发现它了,并把它入队到队列末尾。如果顶
点的颜色是白色,表示还没有发现它,将按照同样的方法继续处理队列中的下一个邻接顶点。    

一旦所有的邻接顶点都已经找到,就将队列头的顶点出队并将其涂黑,表示我们已经完成了对其的查找。
我们继续这个步骤直到队列为空,此时从起始顶点开始可达的所有顶点都已经涂黑了。     

也就是这个意思把，首先，所有点都是白的，代表都没发现。然后，首先把起始点入队，颜色置灰，表示
发现了节点但还没有搜索其邻接点。然后搜索队首元素的邻接点，把邻接点一个个入队且置灰，这里需要
注意的一点是如果一个节点已经灰了，代表已经在队列中，这个邻接点就不入队了（书上没说，但我
觉得如果发现了黑色的节点，也不要入队）。    

搜索完所有的邻接点后，队首出队，置黑。处理下一个队列元素。直到队列为空把。    

## 深度优先搜索

深度优先搜索在搜索过程中每当访问到某个顶点后,需要递归地访问此顶点的所有未访问过的相邻
顶点。因而,这种搜索将尽可能深地持续探索,直到无法继续为止。这种策略使得深度优先搜索
在很多应用中非常有用,包括环检测以及拓扑排序。    

# 第 12 章 排序和搜索

总的来说，排序算法分为两大类：比较排序和线性时间排序。比较排序依赖于比较和交换来将元素移动
到正确的位置上。令人惊讶的是，并不是所有的排序算法都依赖于比较。对于那些确实依赖于比较
来进行排序的算法来说，它们的运行时间往往可能小于 O(nlogn)。对于线性时间排序，从它的名字
就可以看出，它的运行时间往往与它处理的数据元素格式成正比，即为 O(n)。依赖的是，线性
时间排序依赖于数据集合中的某些特征，所以我们并不是在所有的场合都能够使用它。    

+ 插入排序：插入排序虽然不是最有效的排序方法,但它简单,并且不需要额外的存储空间。其最佳应用
场景是对一个小的数据集合进行递增排序。
+ 快速排序：在一般情况下,一致认为快速排序是最好的一种排序算法,而且不需要额外的存储空间。
其最佳应用场合是应用于大型数据集。
+ 归并排序：归并排序基本上与快速排序算法的性能相同,但它需要使用两倍于快速排序的存储空间。
而具有讽刺意味的是,其最佳应用场合是在超大数据集中,因为归并排序的原理就是对原始的乱序数据
不断进行对半分割。
+ 计数排序：计数排序是一种稳定的线性时间排序算法,当知道数据集中整数的最大值的情况下会经常
用到此算法。它主要用来实现基数排序。
+ 基数排序：基数排序是逐位对元素进行排序的线性时间排序算法。基数排序适用于固定大小的元素集,
并且其中的元素易于分割,且易于用整数表示。

## 插入排序

正式的表述为,插入排序每次从无序数据集中取出一个元素,扫描已排好序的数据集,并将它插入
有序集合合适的位置上。    

插入排序是一种较为简单的算法,但是它在处理大型数据集时并不高效。因为很明显,在决定将元素插入
哪个位置之前,需要将被插入元素和有序数据集中的其他元素进行比较,这会随着数据集的增大而增加
额外的开销。然而插入排序的优点是,当将元素插入一个有序数据集中时,只需要对有序数据集最多进行
一次遍历,而不需要完整地运行算法。这个特性使得插入排序在增量排序中非常高效。这种情况可能会发
生,例如:在某酒店预订系统中。假设系统按姓名列出了所有顾客的名单,同时当有新顾客入住时系统会
实时更新。在这种情况下使用插入排序,系统只需要扫视一遍数据,并将新顾客姓名插入列表中,就完成重
新排序。     

这里其实有点不理解，首先，当我们把一个新元素插入有序数据集时，为什么还要使用排序算法，难道
不都是扫描数据集插入到正确位置就行了？其实所谓的增量排序是什么？    

**总结**：O(n^2) 的复杂度，不需要额外存储空间。当在递增排序中使用插入排序时,其时间复杂度为O(n)。  

## 快排

同插入排序一样,快速排序也属于比较排序的一种,而且不需要额外的存储空间。   

这里书上说不需要额外的存储空间，但在我们的实现中确实用到了额外的数组空间来存放大于 pivot
小于 pivot 的元素集合。如果向书上说的不需要额外的存储空间的话，推断应该是使用双指针
来回交换大小元素的位置。   

让我们看看人工对一堆作废的支票进行排序的例子,可将未排序的支票分为两堆。其中一堆专门用于放
小于或等于某个编号的支票,而另一堆用来放大于这个编号的支票(我们认为这个编号大概是所有支票编
号的中间值)。当以这种方式得到两堆支票后,又可以用同样的方式将它们分为四堆,不断重复这个过程
直到每个堆中只放有一张支票。这时,所有支票就已经排好序了。    

令人惊讶的是,考虑到其流行程度,快速排序最坏情况下的性能不会比插入排序的最坏情况好。而事实上,
通过一点点修改,可以大大改善快速排序最坏情况的效率,使其表现得与其平均情况相当。怎样做到
这一点,关键取决于如何选择分割值。    

选择分割值的一种行之有效的方法是通过随机选择法来选取。据统计,即便试图故意让算法陷入瘫痪,随机
选择也能够有效地防止被分割的数据极度不平衡。同时,还可以改进这种随机选择方法,方法是首先
随机选择三个元素,然后选择三个元素中的中间值。这就是所谓的中位数方法,可以保证平均情况下的性能。   

**总结**：复杂度 O(nlogn)，不需要额外空间。   

## 归并排序

归并排序是另一种运用分治法排序的算法。与快速排序一样,它依赖于元素之间的比较来排序。但是,归并
排序需要额外的存储空间来完成排序过程。    

我们再回顾那个对一堆作废的支票进行排序的例子。首先,将这堆未排序的废旧支票对半分为两堆。接着,
分别又将两堆支票对半分为两堆,以此类推,重复此过程,直到每一堆支票只包含一张支票。一旦所
有的堆都只包含一张支票,就开始将堆两两合并,这样每个合并出来的堆就是两个有序堆的合集,也是有序的。
这个合并过程一直持续下去,直到一堆新的支票生成。此时,这堆支票就是有序的。    

归并排序需要额外的存储空间来运行,这也是它的一个缺点。因为合并过程不能在无序数据集本身中进行,
所以必须要有两倍于无序数据集的空间来运行算法。这点不足极大地降低实际中使用归并排序的频率,因为
通常可以使用不需要额外存储空间的快速排序来代替它。   

**总结**：复杂度 O(nlogn)，需要两倍的额外空间。   

## 计数排序

计数排序是一种高效的线性排序，它通过计算一个集合中元素出现的次数来确定集合如何排列。计数排序
不需要进行元素比较,而且它的运行效率要比效率为O(nlogn)比较排序高。   

计数排序有一定的局限性。其中最大的局限就是它只能用于整型或者那些可以用整型来表示的数据集合。
这是因为计数排序利用一个数组的索引来记录元素出现的次数。例如,如果整数3出现过4次,那么4将
存储到数组索引为3的位置上。同时,我们还需要知道集合中最大整数的值,以便于为数组分配足够的空间。   

根据书上的意思，计数排序是这样工作的，首先，我们需要开辟两块额外的空间，一块大小为 k，一块
大小为 n。大小为 k 的 counts 数组用来保存集合中每个元素出现的次数，大小为 n 的 temp
数组用来保存一步步构建的已排序的数组。    

首先扫描一遍集合，将集合中的每个数字出现的次数统计在 counts 数组中，例如一个集合中数字 3
出了 5 次，那么在 counts 中索引为 3 的位置的值就是 5，但这里其实书上没有提到如果出现负值
怎么办。扫描后，我们还需要进行累加操作，具体就是 counts 数组中的每个元素值要重新修改为
当前值与前一个元素值的累加值。这种情况下，其实就可以按顺序得出每个数字最后在 temp 中的偏移，
注意书上说偏移，但是其实好像是位置，例如偏移算下来是 4，那么数字实际上 temp 中该摆放的索引
是 3。    

在进行完累加操作后，逆序再扫描一次集合，每次扫描一个值，找出其在 counts 数组中的值，就是其
在 temp 中的索引，例如，当前最后一个元素是 4，而 counts 中索引为 4 的位置上的值是 8，那么
就在 temp 索引为 7 的位置摆放元素 4，之后 counts 中索引为 4 上的 8 要减 1 变为 7.    

注意这里逆序扫描一次的目的好像是为了原地排序，即不打乱其在原先集合中的位置。    

**总结**：复杂度 O(n + k), n为待排序的元素个数，k 为数据中最大的整数加 1.而根据上面的
情况来看，也需要额外的 n + k 的存储空间。   

从算法的描述上来看，首先，待排序集合必须都是整数，而且是非负整数，或者说都可以转化为非负整数，
其次，在排序前我们就必须知道其中最大的元素值，便于开辟 counts 数组。    

## 基数排序

基数排序是另外一种高效的线性排序算法。其方法是将数据按位分开,并从数据的最低有效位到最高
有效位进行比较,依次排序,从而得到有序数据集合。我们来看一个例子,用基数排序对十进制数据
{15,12,49,16,36,40}进行排序。在对个位进行排序之后,其结果为 {40,12,15,16,36,49},
在对十位进行排序之后,其结果为{12,15,16,36,40,49}。    

有一点非常重要,在对每一位数值进行排序时其排序过程必须是稳定的。因为,一旦一个数值通过较低
有效位的值进行排序之后,此数据的位置不应该改变,除非通过较高有效位的值进行比较后需要它调整位
置。例如,在以上的例子中,整数12和15的十位数都包含1,当对其十位数进行排序时,一个不稳定的排序
算法可能不会维持其在个位数排序过程中的顺序。而一个稳定的排序算法可以保证它们不重新排序。基数
排序会用到计数排序,因为对于基数排序来说,除了稳定性,它还是一种线性算法,且必须知道每一位可能的
最大整数值。    

基数排序并不局限于对整型数据进行排序,只要能把元素分割成整型数,就可以使用基数排序。    

**总结**：复杂度 O(pn + pk)，n 为要排序的元素的个数，k 为基数，p 为位的个数。   


## 问与答

问:假设有一个全球性投资公司,需要将其客户按照名字排序。由于数据量巨大,因此我们不可能一次将
所有数据读入内存。这种情况下应该选择哪种排序算法?    

答:归并排序。除了它的排序时间可以维持在O(nlogn)之外,它以可预见的方式合并有序分区和数据,
这种特性让我们很容易管理数据,使这些数据可以在内存中高效地读入读出。     

总结一下：   

+ 插入排序，复杂度 O(n^2)，有序集合下为 O(n)，不需要额外空间，稳定排序
+ 快排，复杂度 O(nlogn),最差 O(n^2)，不需要额外空间，不稳定
+ 归并，复杂度 O(nlogn)，需要 2 倍额外空间，不稳定？
+ 计数排序，复杂度 O(n + k)，需要 n+k 的额外空间，稳定
+ 基数排序，复杂度 O(pn+pk)，额外空间不清楚，稳不稳定不清楚
+ 冒泡，O(n^2)，稳定，不需要额外空间
+ 选择排序，O(logn)，这里的选择好像和我们平时说的不一样，首先我们平时说的那种选择排序好像是
O(n^2) 的复杂度，而且也不需要额外空间，这里说需要 3 倍的额外空间
+ 堆排序，复杂度 O(nlogn)，不需要额外存储空间，平均情况下要比快排慢一个常数因子
+ 桶排序，线性排序算法     

# 第 14 章 数据压缩


为了理解为什么数据是可以压缩的,我们首先要知道,根据信息的内容所有的数据都会表现出一定的特性,
称为熵(从热力学借用来的一个术语)。压缩是可能的,因为绝大多数的数据所表现出来的容量往往
大于其熵所建议的最佳容量。为了衡量压缩的效率,通常用1减去压缩数据大小除以原始数据大小的值。
这个值称为数据的压缩率。     

从广义上讲,数据压缩的方法分为两大类:有损压缩和无损压缩。在有损压缩中,我们接受数据有一定的
损失来换取更大的压缩比。在某些应用中,一定的损失是可接受的,例如,图像处理和音频处理,因
为,这种损失不会影响其效果并且会受到严格控制。然而,我们通常使用的是无损压缩,它能保证解压缩时
准确地还原原始数据。    

实现无损压缩主要有两种方法:最小冗余编码和基于字典的方法。最小冗余编码使用更少的位对出现
更为频繁的字符进行编码,用较长的位对出现频率较低的字符进行编码。在基于字典的方法中,其通过
对数据进行符号编码,来代替那些重复多余的短语。    

## 位操作

### bit get

bit_get 操作获取缓冲区中一个位的状态，要做到这点，首先要确定位所在的字节，然后通过一个掩码
从字节中获取相应的位。掩码中设为 1 的位是将要从字节中读出的位。接着用一个循环操作将位移动
到适当的位置，通过索引 bits 中相应的字节，并应用掩码，可以获取所需的位。   

bit_get的时间复杂度为O(1)。这是因为获取缓冲区中位的状态所进行的所有操作都能够在固定时间内完成。    

```c
// 看起来还是比较好理解的
// 首先 bits 是指向一个字符串的指针，pos 是要获取位的位置，从 0 开始
int bit_get(const unsigned char *bits, int pos) {
  unsigned char    mask;
  int                i;

  // 一个单字节的掩码
  mask = 0x80

  // 将掩码中的 1 的位移动到对应 pos 在其自身字节中的位置
  // pos % 8 可以计算出我们要取的位在字节中的位置
  for (i = 0; i <(pos % 8); i++) {
    mask = mask >> 1;
  }

  // bits[(int)(pos / 8)] 获取位所在的字节，然后和移位后的掩码进行与运算
  return (((mask & bits[(int)(pos / 8)]) == mask) ? 1 : 0);
}
```    

### bit set

基本上同 bit_get。    

```c
int bit_set(const unsigned char *bits, int pos, int state) {
  unsigned char  mask;
  int     i;

  mask = 0x80;
  for (i = 0; i < (pos % 8); i++) {
    mask = mask << 1;
  }

  if (state) {
    // 如果是置位操作，和掩码或一下即可，将对应为改为 1
    bits[pos/8] = bits[pos/8] | mask;
  } else {
    // 如果是清空操作，先把掩码按位取反一下，再一与，原字节中所有位不变，指定位为 0
    bits[pos/8] = bits[pos/8] & (~mask);
  }
}
```   

